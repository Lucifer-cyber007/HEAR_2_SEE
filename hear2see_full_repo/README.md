Hear2See â€” Audio â†’ Prompt â†’ Video ğŸ¬ğŸ”Šâ†’ğŸ¥

Hear2See converts audio into short AI-generated videos. It transcribes uploaded/recorded audio using an offline Vosk ASR model, converts the transcript into a text prompt (editable), and then generates frames with a text-to-image diffusion model (Stable Diffusion 1.5 + optional LoRA). Frames are stitched into an MP4 video.

Features

Offline transcription using Vosk (no external API).

Editable prompt interface: auto-generate prompt from transcript, then edit.

Text â†’ video generation via Stable Diffusion (fused SD1.5) with LoRA support.

Save input audio, generated frames, and final MP4 files to Drive / disk.

Colab-friendly or local development (GPU recommended for generation).

Gradio frontend (Blocks) with step-by-step flow (Transcribe â†’ Edit â†’ Generate).

Slow-mode option (halves FPS for clearer frame viewing).

Metadata logging (per-run JSONL).
ğŸ“ Repository Structure
hear2see/
â”œâ”€â”€ README.md
â”œâ”€â”€ requirements.txt
â”œâ”€â”€ .gitignore
â”œâ”€â”€ app/
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ ui_frontend.py           # Gradio UI (Blocks) â€” main frontend
â”‚   â”œâ”€â”€ transcription.py         # Vosk logic: load model, transcribe, save audio
â”‚   â”œâ”€â”€ video_generation.py      # SD + LoRA rendering: frames â†’ MP4
â”‚   â”œâ”€â”€ storage.py               # Save audio/video/frames + metadata
â”‚   â”œâ”€â”€ utils.py                 # helpers: prompt builder, audio converters
â”‚   â””â”€â”€ launcher.py              # robust Gradio launcher for Colab/local
â”œâ”€â”€ models/
â”‚   â””â”€â”€ vosk-model-small-en-us-0.15/    # Vosk official small model (0.15)
â”œâ”€â”€ diffusion/
â”‚   â”œâ”€â”€ sd_fused_model/          # fused SD1.5 model dir (user-provided)
â”‚   â””â”€â”€ lora_out/                # LoRA weights (optional)
â”œâ”€â”€ data/
â”‚   â”œâ”€â”€ input_audio/
â”‚   â”œâ”€â”€ frames/
â”‚   â”œâ”€â”€ output_videos/
â”‚   â””â”€â”€ metadata.jsonl
â”œâ”€â”€ notebooks/
â”‚   â”œâ”€â”€ Hear2See_Frontend_Colab.ipynb
â”‚   â””â”€â”€ Diagnostics_Vosk.ipynb
â”œâ”€â”€ scripts/
â”‚   â”œâ”€â”€ download_vosk.sh
â”‚   â””â”€â”€ convert_audio.sh
â”œâ”€â”€ server/
â”‚   â””â”€â”€ main.py                  # optional FastAPI server
â””â”€â”€ assets/
    â”œâ”€â”€ logo.png
    â””â”€â”€ sample_audio/


ğŸ”§ Installation
1ï¸âƒ£ Clone the repository
git clone https://github.com/YOUR-USERNAME/hear2see.git
cd hear2see

2ï¸âƒ£ Install dependencies
pip install -r requirements.txt

3ï¸âƒ£ Download Vosk 0.15 model
bash scripts/download_vosk.sh

4ï¸âƒ£ (Optional) Add Stable Diffusion fused model

Copy your fused SD-1.5 model into:

diffusion/sd_fused_model/

â–¶ï¸ Running the App
ğŸ”Œ Run locally
python app/launcher_local.py

ğŸŒ Running in Google Colab

Just run the notebook:

notebooks/Hear2See_Frontend_Colab.ipynb


The launcher automatically:

detects free ports

sets Colab networking options

supports share=True public UI

ğŸ” Workflow

Upload/Record Audio

Vosk ASR â†’ Transcript

Prompt Builder â†’ Style Selection

Stable Diffusion â†’ Generate Frames

FFmpeg â†’ MP4 Video

Save Audio, Video, Metadata

Outputs appear in:

data/input_audio/
data/output_videos/
data/metadata.jsonl

âš¡ Tips & Recommendations

For video generation, use GPU runtime (A100 recommended).

Replace the fused SD model with any compatible SD1.5 folder.

LoRA training notebook included if you want custom video styles.

ğŸ“œ License

MIT License Â© 2025 Hear2See Team
