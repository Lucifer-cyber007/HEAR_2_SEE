ğŸ¬ğŸ”Š Hear2See â€” Audio â†’ Prompt â†’ Video (Offline + Diffusion)

Hear2See is a fully offline, modular system that converts audio â†’ transcript â†’ video, combining:

Vosk 0.15 Offline ASR

Stable Diffusion (fused SD-1.5) with optional LoRA

Gradio UI frontend

Automatic audio/video saving + metadata logging

Colab-friendly launcher

This repository includes a clean, production-ready modular codebase and a complete folder structure for running and extending Hear2See.

ğŸš€ Features
ğŸ”Š Speech â†’ Text (Offline)

Uses Vosk 0.15 small model (40MB)

No internet required

Converts any uploaded/recorded audio (WAV, MP3, M4A, OGG, FLAC)

Auto-normalizes audio to 16kHz WAV internally

ğŸ“ Text â†’ Prompt

Automatic prompt builder (cinematic / anime / realistic)

Users can edit transcript before generating video

ğŸ¥ Text â†’ Video (Diffusion)

Uses Stable Diffusion 1.5 fused model (local folder)

Frame-by-frame rendering

Video stitching using ffmpeg

Slow Mode (Â½ FPS)

ğŸ’¾ Storage Engine

Saves uploaded audio to data/input_audio/

Saves videos to data/output_videos/

Stores frames optionally

Logs all runs to data/metadata.jsonl

ğŸ–¥ï¸ UI & Deployment

Gradio Blocks UI (fully responsive)

Robust launcher for Colab (port management, queue compatibility)

Local launcher for desktop use

ğŸ“ Repository Structure
hear2see/
â”‚
â”œâ”€â”€ README.md
â”œâ”€â”€ requirements.txt
â”œâ”€â”€ .gitignore
â”‚
â”œâ”€â”€ app/
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ ui.py                      # Full Gradio frontend (Blocks)
â”‚   â”œâ”€â”€ transcription.py           # Vosk ASR logic
â”‚   â”œâ”€â”€ video_generation.py        # Stable Diffusion rendering backend
â”‚   â”œâ”€â”€ storage.py                 # Audio/video saving + metadata
â”‚   â”œâ”€â”€ utils.py                   # Prompt builder + helpers
â”‚   â””â”€â”€ launcher_local.py          # Local/Colab safe launcher
â”‚
â”œâ”€â”€ models/
â”‚   â””â”€â”€ vosk-model-small-en-us-0.15/   # Official Vosk ASR model directory
â”‚
â”œâ”€â”€ diffusion/
â”‚   â”œâ”€â”€ sd_fused_model/            # Stable Diffusion 1.5 model folder (user provided)
â”‚   â””â”€â”€ lora_out/                  # (Optional) LoRA weights + training logs
â”‚       â”œâ”€â”€ pytorch_lora_weights.safetensors
â”‚       â”œâ”€â”€ training_args.json
â”‚       â”œâ”€â”€ logs/
â”‚       â””â”€â”€ samples/
â”‚
â”œâ”€â”€ data/
â”‚   â”œâ”€â”€ input_audio/               # All saved audio files
â”‚   â”œâ”€â”€ output_videos/             # All generated videos
â”‚   â”œâ”€â”€ frames/                    # (Optional) saved individual frames
â”‚   â””â”€â”€ metadata.jsonl             # One JSON record per generation
â”‚
â”œâ”€â”€ notebooks/
â”‚   â”œâ”€â”€ Hear2See_Frontend_Colab.ipynb     # Full working Colab notebook
â”‚   â”œâ”€â”€ LoRA_Training_Notebook.ipynb      # For fine-tuning LoRA
â”‚   â”œâ”€â”€ Diagnostics_Vosk.ipynb            # For fixing/validating ASR models
â”‚   â””â”€â”€ Experiments/
â”‚
â”œâ”€â”€ scripts/
â”‚   â”œâ”€â”€ download_vosk.sh             # Script to download Vosk 0.15 small model
â”‚   â”œâ”€â”€ convert_audio.sh             # Batch 16kHz conversion helper
â”‚   â””â”€â”€ train_lora.sh                # CLI LoRA training boilerplate
â”‚
â”œâ”€â”€ server/
â”‚   â”œâ”€â”€ main.py                      # (Optional) FastAPI backend
â”‚   â””â”€â”€ api/
â”‚       â”œâ”€â”€ transcription_api.py
â”‚       â”œâ”€â”€ video_api.py
â”‚       â””â”€â”€ health_check.py
â”‚
â””â”€â”€ assets/
    â”œâ”€â”€ logo.png
    â”œâ”€â”€ sample_audio/
    â””â”€â”€ sample_output/

ğŸ”§ Installation
1ï¸âƒ£ Clone the repository
git clone https://github.com/YOUR-USERNAME/hear2see.git
cd hear2see

2ï¸âƒ£ Install dependencies
pip install -r requirements.txt

3ï¸âƒ£ Download Vosk 0.15 model
bash scripts/download_vosk.sh

4ï¸âƒ£ (Optional) Add Stable Diffusion fused model

Copy your fused SD-1.5 model into:

diffusion/sd_fused_model/

â–¶ï¸ Running the App
ğŸ”Œ Run locally
python app/launcher_local.py

ğŸŒ Running in Google Colab

Just run the notebook:

notebooks/Hear2See_Frontend_Colab.ipynb


The launcher automatically:

detects free ports

sets Colab networking options

supports share=True public UI

ğŸ” Workflow

Upload/Record Audio

Vosk ASR â†’ Transcript

Prompt Builder â†’ Style Selection

Stable Diffusion â†’ Generate Frames

FFmpeg â†’ MP4 Video

Save Audio, Video, Metadata

Outputs appear in:

data/input_audio/
data/output_videos/
data/metadata.jsonl

âš¡ Tips & Recommendations

For video generation, use GPU runtime (A100 recommended).

Replace the fused SD model with any compatible SD1.5 folder.

LoRA training notebook included if you want custom video styles.

ğŸ“œ License

MIT License Â© 2025 Hear2See Team
